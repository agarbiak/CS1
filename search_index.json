[
["index.html", "Actuarial Statistics Introduction Motivation", " Actuarial Statistics Alex Garbiak 17 October, 2020 Introduction This book is unpublished and should be considered an early build version. There will be bugs, typos and errors. Please report them here. Motivation The purpose of this book is to help actuaries studying Actuarial Statistics (CS1) from the Institute and Faculty of Actuaries by providing R code examples across the following topic areas: Random variables and distributions Data analysis Statistical inference Regression theory and applications Bayesian statistics "],
["r-setup.html", "Chapter 1 R Setup 1.1 Preparing your environment for R 1.2 Basic interations with R 1.3 Functions in R 1.4 Data structures in R 1.5 Logical expressions in R 1.6 Extending R with packages 1.7 Importing data", " Chapter 1 R Setup 1.1 Preparing your environment for R The Institute and Faculty of Actuaries have provided their own guide to getting up and running with R. The steps to have R working is dependant on your operating system. The following resources should allow for your local installation of R to be relatively painless: Download and install R from CRAN1. Download and install an integrated development environment, a strong recommendation is RStudio Desktop. 1.2 Basic interations with R R is case-sensitive! We add comments to our R code using the # symbol on any line. A key concept when working with R is that the preference is to work with vectorised operations (over concepts like for loops). As an example we start with 1:10 which uses the colon operator (:) to generate a sequence starting with 1 and ending with 10 in steps of 1. The output is a numeric vector of integers. Let’s see this in R: # This is the syntax for comments in R (1:10) + 2 # Notice how we add element-wise in R ## [1] 3 4 5 6 7 8 9 10 11 12 At the most basic level, R vectors can be of atomic modes: integer, numeric (equivalently, double), logical which take on the Boolean types: TRUE or FALSE and can be coerced into integers as 1 and 0 respectively, character which will be apparent in R with the wrapper \"\", complex, and raw This book focuses on using R to solve actuarial statistical problems and will not explore the depths of the R language2. R has the usual arithmetic operators you’d expect with any programming language: +, -, *, / for addition, subtraction, multiplication and division, ^ for exponentiation, %% for modulo arithmetic (remainder after division) %/% for integer division We assign values to variables using the &lt;- (“assignment”) operator3. x &lt;- 1:10 y &lt;- x + 2 x &lt;- x + x # Notice that we can re-assign values to variables z &lt;- x + 2 y ## [1] 3 4 5 6 7 8 9 10 11 12 z ## [1] 4 6 8 10 12 14 16 18 20 22 Even though \\(z\\) is assigned the same way as we assigned \\(y\\), note that \\(y \\neq z\\) so execution order matters in R. All of \\(x\\), \\(y\\) and \\(z\\) are vectors in R. 1.3 Functions in R We can add functions to R via the format function_name(arguments = values, ...): # c() is the &quot;combine&quot; function, used often to create vectors # Note we can also nest functions within functions x &lt;- c(1:3, 6:20, 21:42, c(43, 44)) # Another function with arguments: y &lt;- sample(x, size = 3) y ## [1] 11 41 42 There are a lot of in-built functions in R that we may need: factorial(x) choose(n, k) - for binomial coefficients exp(x) log(x) - by default in base \\(e\\) gamma(x) abs(x) - absolute value sqrt(x) sum(x) mean(x) median(x) var(x) sd(x) quantile(x, 0.75) set.seed(seed) - for reproducibility of random number generation sample(x, size) R has an in-built help function ? which can be used to read the documentation on any function as well as topic areas. For example have a look at ?Special for more details about in-built R functions for the beta and gamma functions. 1.4 Data structures in R We have already seen vectors as a data structure that is very common in R. We can identify the structure of an R “object” using the str(object) function. Matrices Next we introduce the matrix structure. When interacting with matrices in R it is important to note that matrix multiplication requires the %*% syntax: first_matrix &lt;- matrix(1:9, byrow = TRUE, nrow = 3) first_matrix %*% first_matrix ## [,1] [,2] [,3] ## [1,] 30 36 42 ## [2,] 66 81 96 ## [3,] 102 126 150 Dataframes A data.frame is a very popular data structure used in R. Each input variable has to have the same length but can be of different types (strings, integers, booleans, etc.). # Input vectors for the data.frame name &lt;- c(&quot;Mercury&quot;, &quot;Venus&quot;, &quot;Earth&quot;, &quot;Mars&quot;, &quot;Jupiter&quot;, &quot;Saturn&quot;, &quot;Uranus&quot;, &quot;Neptune&quot;) surface_gravity &lt;- c(0.38, 0.904, 1, 0.3794, 2.528, 1.065, 0.886, 1.14) # Create a data.frame from the vectors solar_system &lt;- data.frame(name, surface_gravity) str(solar_system) ## &#39;data.frame&#39;: 8 obs. of 2 variables: ## $ name : chr &quot;Mercury&quot; &quot;Venus&quot; &quot;Earth&quot; &quot;Mars&quot; ... ## $ surface_gravity: num 0.38 0.904 1 0.379 2.528 ... Lists A list is a versatile data structure in R as their elements can be of any type, including lists themselves. In fact a data.frame is a specific implementation of a list which allows columns in a data.frame to have different types, unlike a matrix. We will come across a number of functions that return a list type whilst working with actuarial statistics in R. For example when we look at linear models we will make use of the lm(formula, data, ...) function which returns a list. # Use Orange dataset df &lt;- Orange # Fit a linear model to predict circumference from age fitted_lm &lt;- lm(circumference ~ age, df) # Size of the list length(fitted_lm) ## [1] 12 # Element names names(fitted_lm) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; We can access elements in the list using subsetting, noting the use of the [[ operator. Here we subset on “age” within the “coefficient” element in the list we called “fitted_lm”: # Select [[1]] 1st element in the list, sub-select [2] 2nd element from that fitted_lm[[1]][2] ## age ## 0.1067703 # fitted_lm$coefficient is a shorthand for fitted_lm[[&quot;coefficient&quot;]] fitted_lm$coefficients[2] ## age ## 0.1067703 # Select element using matching character vector &quot;age&quot; fitted_lm$coefficients[&quot;age&quot;] ## age ## 0.1067703 # Select elements using matching character vectors fitted_lm[[&quot;coefficients&quot;]][&quot;age&quot;] ## age ## 0.1067703 1.5 Logical expressions in R R has built in logic expressions: Operator Description &lt; (&lt;=) less than (or equal to) &gt; (&gt;=) greater than (or equal to) == exactly equal to ! NOT &amp; AND (element-wise) | OR (element-wise) != not equal to We can use logical expressions to effectively filter data via subsetting the data using the [...] syntax: x &lt;- 1:10 x[x != 5 &amp; x &lt; 7] ## [1] 1 2 3 4 6 We can select objects using the $ symbol (see ?Extract for more help): #data.frame[rows to select, columns to select] solar_system[solar_system$name == &quot;Jupiter&quot;, c(1:2)] ## name surface_gravity ## 5 Jupiter 2.528 1.6 Extending R with packages We can extend R’s functionality by loading packages: # Load the ggplot2 package library(ggplot2) Did you get an error from R trying this? To load packages they need to be installed using install.packages(&quot;package name&quot;). 1.7 Importing data R can import a wide variety of file formats, including: .csv .RData .txt We can import these using read.csv(), load() and read.table() respectively. CRAN is the The Comprehensive R Archive Network - read more on the CRAN website↩︎ I fear this is already too indepth for “basic interactions with R” but for those that want to jump down the rabbit hole, see Hadley Wickham’s book Advanced R.↩︎ We can also assign values using the more familiar = symbol. In general this is discouraged, listen to Hadley Wickham.↩︎ "],
["discrete-probability-distributions.html", "Chapter 2 Discrete Probability Distributions Learning Objectives Theory 2.1 In-built probability distributions Discrete probability distributions covered 2.2 Geometric distribution 2.3 Binomial distribution 2.4 Negative binomial distribution 2.5 Hypergeometric distribution 2.6 Poisson distribution 2.7 Uniform distribution R Practice", " Chapter 2 Discrete Probability Distributions Learning Objectives Define and explain the key characteristics of the discrete distributions: geometric, binomial, negative binomial, hypergeometric, Poisson and uniform on a finite set. Evaluate probabilities and quantiles associated with such distributions. Theory R was designed to be used for statistical computing - so it handles randomness well! Using R we can guarantee reproducibility (and enhance sharability) by using the function set.seed(seed) where seed is a single value integer. Using this approach we guarantee the generation of the same sequence of random numbers everytime we call this function. Use ?set.seed to learn more about this function. Let’s see set.seed in action: # We make five random draws from the integer range [1, 10] # We cannot guarantee reproducing this outcome when sharing the code: sample(1:10, 5) ## [1] 3 5 1 8 7 # Now we set a seed value before making the five random draws # We guarantee a fixed output which enhances reproducibility and sharability: set.seed(42) # Fixes result # Using set.seed(42) we guarantee five random draws within the integer range [1, 10] will be: # 1, 5, 10, 8, 2 sample(1:10, 5) ## [1] 1 5 10 8 2 # We can re-initialise the seed with the same value # Observe that the same sequence of random numbers are generated: set.seed(42) # Fixes result # Guarantee we draw 1, 5, 10, 8, 2 again sample(1:10, 5) ## [1] 1 5 10 8 2 2.1 In-built probability distributions R has in-built functions for probability distributions: d&lt;distribution-name&gt; \\(:=\\) density (“PDF”), i.e. \\(f_X(x)\\) p&lt;distribution-name&gt; \\(:=\\) probability distribution cumulative function (“CDF”), i.e. \\(F_X(x) =\\boldsymbol{P}(X \\leq x)\\) q&lt;distribution-name&gt; \\(:=\\) quantile function, i.e. return \\(x\\) such that \\(\\boldsymbol{P}(X \\leq x) = p\\) r&lt;distribution-name&gt; \\(:=\\) random deviates, i.e. (psuedo) random number generator for a given distribution Where &lt;distribution-name&gt; \\(=\\) Normal, uniform, lognormal, Student’s \\(t\\), Poisson, binormal, Weibull … see ?distributions() for more information To give some quick examples (we will explore these in more detail later in this chapter and the next chapter): R Code Definition rnorm(1) Generates \\(x_1\\) where \\(X \\sim \\mathcal{N}(0,\\,1)\\) rnorm(y, mean=10, sd=2) Generates \\(\\{y_1,\\,y_2,\\,\\dots\\}\\) with \\(Y \\sim \\mathcal{N}(10,\\,2^2)\\) runif(3, min=5, max=10) Generates \\(\\{z_1,\\,z_2,\\,z_3\\}\\) where \\(Z \\sim \\mathcal{U}(5,\\,10)\\) dbinom(4, size=5, prob=0.5) Computes \\(\\boldsymbol{P}(X = 4)\\) where \\(X \\sim Bin(5,\\,0.5)\\) pgamma(0.2, shape=2, rate=2) Computes \\(F_Y(0.2)\\) where \\(Y \\sim \\mathcal{\\Gamma}(2,\\,2)\\), i.e. \\(\\boldsymbol{P}(Y\\leq 0.2)\\) qexp(0.5, rate = 2) Determines smallest value of \\(z\\) for \\(\\boldsymbol{P}(Z \\leq z) = 0.5\\) where \\(Z \\sim Exp(2)\\) Discrete probability distributions covered We will consider how to interact with the following discrete probability distributions in R: Geometric Binomial Negative binomial Hypergeometric Poisson Uniform (on a finite set) For each distribution above we will determine how to calculate: A random deviate following the discrete distribution \\(X\\), The probability mass function (“PMF”), \\(P(X = k)\\) for distribution \\(X\\) and \\(-\\infty &lt; k &lt; \\infty\\) (noting the PMF \\(= 0\\) for most values of \\(k\\)), The cumulative distribution function (“CDF”), \\(P(X \\leq k)\\), A range of PMFs, i.e. \\(P(k_1 \\leq X \\leq k_2)\\), and The quantile function to find \\(k\\) representing the value such that \\(P(X \\leq k) = p\\), i.e. the pth percentile. We will finish off with a plot of the distribution. 2.2 Geometric distribution The geometric probability distribution (\\(X\\)) can be defined by the number of failures (\\(k\\)) in Bernoulli trials before achieving a success. A Bernoulli trial is a random experiment with exactly two outcomes, {“success”, “failure”}, in which the probability of success is constant in every experiment. More formally, if we have \\(k \\in \\{0,\\,1,\\,2,\\, \\dots \\}\\) independent, identically distributed (“i.i.d.”) Bernoulli trials before a “success” with the \\(P(\\)“success”\\()\\) on each trial defined as \\(p\\), then: \\[P(X = k) = (1 - p)^{k}p\\] We have \\(X \\sim Geo(p)\\) with \\(p \\in (0, 1]\\). In R we can generate random deviates following a geometric distribution using the function rgeom(n, prob) where: n is the number of random deviates we want to generate, and prob is the probability of success in any given Bernoulli trial. # Guarantee reproducibility set.seed(42) # Generate 5 random deviates following X~Geo(0.25) rgeom(5, 0.25) ## [1] 1 1 0 1 2 If we wanted to find the xth percentile of \\(X \\sim Geo(p)\\) we would use the quantile function, qgeom(p, prob). # Find the 99th percentile of X~Geo(0.25) percentile_99 &lt;- qgeom(0.99, 0.25) paste0( &quot;The 99th percentile of X~Geo(0.25) is &quot;, percentile_99, &quot;.&quot; ) ## [1] &quot;The 99th percentile of X~Geo(0.25) is 16.&quot; If we wish to find \\(P(X \\leq k)\\) then we need to use the function for the cumulative distribution function, pgeom(q, prob) where: q equates to \\(k\\) failures before success in a sequence of i.i.d. Bernoulli trials, prob is the probability of success in any given Bernoulli trial. # Find P(X &lt;= 7) with X~Geo(0.25) prob_geom &lt;- pgeom(7, 0.25) paste0( &quot;P(X &lt;= 7) with X~Geo(0.25) is &quot;, format(prob_geom, digits = 4), &quot;.&quot; ) ## [1] &quot;P(X &lt;= 7) with X~Geo(0.25) is 0.8999.&quot; We can find \\(P(X = k)\\) using the dgeom() function call: # Find P(X = 7) with X~Geo(0.25) prob_geom &lt;- dgeom(7, 0.25) We find that \\(P(X = 7) =\\) 0.0333709716796875. It helps to visualise the probability distribution. We can do this in base R with the plot() function. However for illustrative purposes we will use the ggplot2 package from the tidyverse suite of packages. ggplot2 allows for rich visualisations4 using a consistent syntax. # Plot the distribution function of X~Geo(0.25) library(ggplot2) df &lt;- data.frame(x = rgeom(1000, 0.25)) ggplot(df, aes(x=x)) + geom_histogram(binwidth = 0.5) 2.3 Binomial distribution The binomial probability distribution (\\(Y\\)) can be defined by the number of successes in a sequence of \\(n\\) i.i.d. Bernoulli trials. A Bernoulli trial is a random experiment with exactly two outcomes, {“success”, “failure”}, wherein the probability of success is \\(p\\). We often state the probability of failure as \\(q = 1 - p\\) for convenience. More formally, if we have \\(k \\in \\{0,\\,1,\\,2,\\, \\dots,\\, n \\}\\) successes given \\(n\\) i.i.d. Bernoulli trials with the \\(P(\\)“success”\\()\\) on each trial defined as \\(p\\), then: \\[P(Y = k) = \\binom{n}{k}p^k(1 - p)^{n - k}\\] It can help to think of this as: \\(p^k\\) chance of \\(k\\) successes, \\((1 - p)^{n - k}\\) chance of \\(n - k\\) failures, the ordering of the \\(k\\) successes can occur anywhere within the \\(n\\) trials, hence \\(\\binom{n}{k}\\). We can generate random deviates following the binomial distribution \\(Y \\sim Bin(n,\\,p)\\) using rbinom(n, size, prob) where: n is the number of random deviates we want to generate, size is the number of i.i.d. Bernoulli trials performed, and prob is the probability of success in any given trial. # Generate 5 random deviates on Y~Bin(10, 0.55) set.seed(42) rbinom(5, 10, 0.55) ## [1] 3 3 6 4 5 To find \\(P(Y \\leq k)\\) we can use the function pbinom(p, size, prob) where: p is the probability of interest, size is the number of i.i.d. Bernoulli trials performed, and prob is the probability of success in any given trial. Suppose we were told the probability of having a boy at birth was 0.51314515. Additionally, suppose we were told that there were 1,264 births6. Given this, we are curious to know what is the probability of there being less than 632 boys (i.e. &lt; 50%) amongst the new babies. # Find P(Y &lt;= 632) with Y~Binom(1264, 0.5131451) prob_binom &lt;- pbinom(632, 1264, 0.5131451) paste0( &quot;P(Y &lt;= 632) with Y~Binom(1264, 0.5131451) is &quot;, format(prob_binom, digits = 4), &quot;.&quot; ) ## [1] &quot;P(Y &lt;= 632) with Y~Binom(1264, 0.5131451) is 0.1822.&quot; To find \\(P(Y = k)\\) we can use the function dbinom(x, size, prob) where: x is the quantile of interest, size is the number of i.i.d Bernoulli trials performed, and prob is the probability of success in any given trial. Throwing a fair coin, we are curious to compute the probability of observing 4 heads in a sequence of 5 tosses: # Find P(Y = 4) where Y~Binom(5, 0.5) prob_binom &lt;- dbinom(4, 5, 0.5) paste0( &quot;P(Y = 4) with Y~Binom(5, 0.5) is &quot;, format(prob_binom, digits = 4), &quot;.&quot; ) ## [1] &quot;P(Y = 4) with Y~Binom(5, 0.5) is 0.1562.&quot; If we wanted to find \\(P(k_1 \\leq Y \\leq k_2)\\) we can also use the function dbinom(x, size, prob) with x entered as a vector and summing over the output. Continuing the fair coin example, we are curious what is the probability of observing at least 1 head and at most 3 heads in a sequence of 5 tosses: # Find P(1 &lt;= Y &lt;= 3) where Y~Binom(5, 0.5) prob_binom &lt;- sum(dbinom(1:3, 5, 0.5)) paste0( &quot;P(1 &lt;= Y &lt;= 3) with Y~Binom(5, 0.5) is &quot;, format(prob_binom, digits = 4), &quot;.&quot; ) ## [1] &quot;P(1 &lt;= Y &lt;= 3) with Y~Binom(5, 0.5) is 0.7812.&quot; When we are interested in finding the yth percentile we can use the quantile function, qbinom(p, size, prob) where: p is the percentile of interest, size is the number of i.i.d Bernoulli trials performed, and prob is the probability of success in any given Bernoulli trial. Continuing the baby boys example, we now are curious what is the 99th percentile of baby births being boys. Recall that the probability (p) of having a boy at birth was 0.5131451 and that there were 1,264 births (n) in Guildford in 2019. # Find the 99th percentile of Y~Binom(1264, 0.5131451) percentile_99 &lt;- qbinom(0.99, 1264, 0.5131451) paste0( &quot;The 99th percentile of Y~Binom(1264, 0.5131451) is &quot;, percentile_99, &quot;.&quot; ) ## [1] &quot;The 99th percentile of Y~Binom(1264, 0.5131451) is 690.&quot; Finally, we finish with a plot of binomial distributions and note that with large \\(n\\) the shape of the distribution begins to form the normal distribution (as long as \\(p\\) is not near the extremes of 0 or 1). # Plot the distribution function of X, Y, Z with: # X,Y,Z~Binom(n, p) for various {n, p} library(ggplot2) set.seed(42) df &lt;- data.frame( dist_types = factor( rep(c( &quot;n = 20, p = 0.5&quot;, &quot;n = 20, p = 0.7&quot;, &quot;n = 40, p = 0.5&quot; ), each = 1000 ) ), dist_values = c( rbinom(1000, 20, 0.5), rbinom(1000, 20, 0.7), rbinom(1000, 40, 0.5) ) ) ggplot(df, aes(x = dist_values, fill = dist_types)) + geom_histogram(binwidth = 1, alpha = 0.75, position = &quot;identity&quot;) + xlab(&quot;Distribution values&quot;) + ylab(&quot;Count&quot;) + theme_minimal() + labs(fill = &quot;&quot;) + ggtitle(&quot;Histogram of binomial distributions for various n, p&quot;) 2.4 Negative binomial distribution The negative binomial distribution (\\(Z\\)) can be defined by the number of failures in a sequence of \\(n\\) independent, identically distributed (“i.i.d.”) Bernoulli trials before a specified number of successes, \\(r\\), occurs. More formally, if we have, \\(r &gt;0\\) number of successes, \\(k\\) number of failures, \\(p\\) probability of success then: \\[P(Z = k) = \\binom{k+r-1}{k}(1-p)^{k}p^{r}\\] We can see that with \\(r = 1\\), the binomial distribution is a special case of the more general negative binomial distribution. Let us begin by generate random deviates from the negative binomial distribution, \\(Z \\sim NegBin(r, p)\\), using rnbinom(n, size, prob) where: n is the number of random deviates we want to generate, size is the target number of successes from i.i.d. Bernoulli trials, and prob is the probability of success in any given trial # Generate random deviates from the negative binomial distribution set.seed(42) rnbinom(5, 10, 0.5) ## [1] 12 6 10 14 16 To find \\(P(Z \\leq k)\\) we can use the function pnbinom(q, size, prob) where: q is the quantile of interest, size is the target number of successes from i.i.d. Bernoulli trials, prob is the probability of success in any given trial. # Find P(Z &lt;= 2) with Z~NegBin(5, 0.5) prob_nbinom &lt;- pnbinom(2, 5, 0.5) paste0( &quot;P(Z &lt;= 2) with Z~NegBin(5, 0.5) is &quot;, format(prob_nbinom, digits = 4), &quot;.&quot; ) ## [1] &quot;P(Z &lt;= 2) with Z~NegBin(5, 0.5) is 0.2266.&quot; To find \\(P(Z = k)\\) we can use the function dnbinom(x, size, prob) where: x is the quantile of interest, size is the target number of successes from i.i.d. Bernoulli trials, and prob is the probability of success in any given trial. # Find P(Z = 2) where Z~NegBin(5, 0.5) prob_nbinom &lt;- dnbinom(2, 5, 0.5) paste0( &quot;P(Z = 2) with Z~NegBin(5, 0.5) is &quot;, format(prob_nbinom, digits = 4), &quot;.&quot; ) ## [1] &quot;P(Z = 2) with Z~NegBin(5, 0.5) is 0.1172.&quot; We can also calculate \\(P(k_1 \\leq Z \\leq k_2)\\) with dnbinom(x, size, prob) by entering x as a vector input and summing over the output. # Find P(1 &lt;= Z &lt;= 3) with Z~NegBin(5, 0.5) prob_nbinom &lt;- sum(dnbinom(1:3, 5, 0.5)) paste0( &quot;P(1 &lt;= Z &lt;= 3) with Z~NegBin(5, 0.5) is &quot;, format(prob_nbinom, digits = 4), &quot;.&quot; ) ## [1] &quot;P(1 &lt;= Z &lt;= 3) with Z~NegBin(5, 0.5) is 0.332.&quot; When we are interested in finding the zth percentile we can make use of the quantile function, qnbinom(p, size, prob) where: p is the percentile of interest, size is the target number of successes from i.i.d. Bernoulli trials, and prob is the probability of success in any given trial. # Find the 99th percentile of Z~NegBin(100, 0.5) percentile_99 &lt;- qnbinom(0.99, 100, 0.5) paste0( &quot;The 99th percentile of Z~NegBin(100, 0.5) is &quot;, percentile_99, &quot;.&quot; ) ## [1] &quot;The 99th percentile of Z~NegBin(100, 0.5) is 135.&quot; Finally, we finish with a plot of the negative binomial distribution: # Plot the distribution function of X, Y, Z with: # X,Y,Z~NegBin(r, p) for various {r, p} library(ggplot2) set.seed(42) df &lt;- data.frame( dist_types = factor( rep(c( &quot;r = 5, p = 0.6&quot;, &quot;r = 5, p = 0.5&quot;, &quot;r = 10, p = 0.4&quot; ), each = 1000 ) ), dist_values = c( rnbinom(1000, 5, 0.6), rnbinom(1000, 5, 0.5), rnbinom(1000, 10, 0.4) ) ) ggplot(df, aes(x = dist_values, fill = dist_types)) + geom_histogram(binwidth = 1, alpha = 0.75, position = &quot;identity&quot;) + xlab(&quot;Distribution values&quot;) + ylab(&quot;Count&quot;) + theme_minimal() + labs(fill = &quot;&quot;) + ggtitle(&quot;Histogram of negative binomial distributions for various r, p&quot;) 2.5 Hypergeometric distribution The hypergeometric distribution (\\(X\\)) can be defined by the probability of \\(k\\) successes in \\(n\\) draws without replacement from a finite population \\(N\\) that contains exactly \\(K\\) “success” objects/states. \\[P(X = k) = \\frac{ \\binom{K}{k} \\binom{N - K}{n - k} }{ \\binom{N}{n} }\\] We begin by generating random deviates from the hypergeometric distribution, \\(X \\sim Hyper(N, K, n)\\), using rhyper(nn, m, n, k) where: nn is number of observations we want to generate, m is number of “success” objects/states in the population \\(N\\), n is number of “failure” objects/states in the population \\(N\\), and k is the number of objects drawn / states observed Traditionally we have defined the population \\(N\\) as an urn containing m white balls and n black balls and this is how the distribution arguments are specified in the stats package. Note that m \\(= K\\) and n \\(= N - K\\). # Generate random deviates on X~Hyper(N = 100, K = 90, n = 10) set.seed(42) rhyper(5, 90, 100 - 90, 10) ## [1] 8 8 10 8 9 In the above random deviate generation we specified a 90% (\\(\\frac{90}{90 + 10}\\)) chance of drawing a “success” object and we made 10 draws from this population of 100 objects. With such a high proportion of “success” objects it was not suprising to note we generated random deviates from this distribution close to 10/10. Next we wish to find \\(P(X \\leq k)\\) which involves using phyper(q, m, n, k) where: q is the quantile of interest (representing the number of “success” objects in the finite population \\(N\\)), m is the number of “success” objects in the population \\(N\\), n is number of “failure” objects in the population \\(N\\), and k is the number of objects drawn # Find P(X &lt;= 5) with X~Hyper(N = 100, K = 90, n = 10) prob_hyper &lt;- phyper(5, 90, 100 - 90, 10) paste0( &quot;With 10 objects drawn from a total 100 possible, &quot;, &quot;P(X &lt;= 3) with X~Hyper() is &quot;, format(prob_hyper, digits = 4), &quot;.&quot; ) ## [1] &quot;With 10 objects drawn from a total 100 possible, P(X &lt;= 3) with X~Hyper() is 0.0006716.&quot; To find \\(P(X = k)\\) we can use the function dhyper(x, m, n, k) where: x is the quantile of interest (representing the number of “success” objects in the finite population \\(N\\)), m is the number of “success” objects in the population \\(N\\), n is number of “failure” objects in the population \\(N\\), and k is the number of objects drawn # Find P(X = 5) with X~Hyper(N = 100, K = 90, n = 10) prob_hyper &lt;- dhyper(5, 90, 100 - 90, 10) We can also calculate \\(P(k_1 \\leq X \\leq k_2)\\) with dhyper(x, m, n, k) by entering x as a vector input and summing over the output. # Find P(7 &lt;= X &lt;= 9) with X~Hyper(N = 100, K = 90, n = 10) prob_hyper &lt;- sum(dhyper(7:9, 90, 100 - 90, 10)) When we are interested in finding the xth percentile we can make use of the quantile function, qhyper(p, m, n, k) where: p is the percentile of interest, m is the number of “success” objects in the population \\(N\\), n is number of “failure” objects in the population \\(N\\), and k is the number of objects drawn # Find the 99th percentile of X~Hyper(N = 100, K = 90, n = 50) percentile_99 &lt;- qhyper(0.99, 90, 100 - 90, 50) paste0( &quot;When we draw 50 objects from a population of &quot;, &quot;90 &#39;success&#39; objects and 10 &#39;failure&#39; objects, &quot;, &quot;we expect the 99th percentile outcome to be &quot;, percentile_99, &quot;.&quot; ) ## [1] &quot;When we draw 50 objects from a population of 90 &#39;success&#39; objects and 10 &#39;failure&#39; objects, we expect the 99th percentile outcome to be 48.&quot; Finally, we finish with a plot of the hypergeometric distribution: # Plot the distribution function of X, Y, Z with: # X,Y,Z~Hyper(N, K, n) for various {N, K, n} library(ggplot2) set.seed(42) inputs &lt;- c( &quot;N = 500, K = 50, n = 100&quot;, &quot;N = 500, K = 60, n = 200&quot;, &quot;N = 500, K = 70, n = 300&quot; ) df &lt;- data.frame( dist_types = rep(inputs, each = 1000), dist_values = c( rhyper(1000, 50, 500 - 50, 100), rhyper(1000, 60, 500 - 60, 200), rhyper(1000, 70, 500 - 70, 300) ) ) ggplot(df, aes(x = dist_values, fill = dist_types)) + geom_histogram(binwidth = 1, alpha = 0.75, position = &quot;identity&quot;) + xlab(&quot;Distribution values&quot;) + ylab(&quot;Count&quot;) + theme_minimal() + labs(fill = &quot;&quot;) + scale_fill_discrete(breaks = inputs) + ggtitle(&quot;Histogram of Hypergeometric distributions for various inputs&quot;) 2.6 Poisson distribution The Poisson distribution (\\(Y\\)) is usually defined as the probability of a given number of events occurring (\\(k\\)) in a fixed interval of time where events occur with a constant mean rate (\\(\\lambda\\)) and independently of the time since the last event. More formally, for \\(\\lambda &gt; 0\\) and \\(k = 0,\\,1,\\,2,\\,\\dots\\,\\): \\[P(Y = k) = \\frac{\\lambda^k e^{-k}}{k!}\\] We can generate random deviates from the Poisson distribution \\(Y \\sim Pois(\\lambda)\\) using rpois(n, lambda) where: n is the number of random deviates we want to generate, and lambda is the constant mean rate. # Generate 5 random deviates on Y~Pois(8) set.seed(42) rpois(5, 8) ## [1] 12 13 6 11 9 To find \\(P(Y \\leq k)\\) we can use the function ppois(q, lambda) where: q is the quantile of interest, and lambda is the constant mean rate. # Find P(Y &lt;= 5) with Y~Pois(8) prob_pois &lt;- ppois(5, 8) paste0( &quot;P(Y &lt;= 5) with Y~Pois(5, 8) is &quot;, format(prob_pois, digits = 4), &quot;.&quot; ) ## [1] &quot;P(Y &lt;= 5) with Y~Pois(5, 8) is 0.1912.&quot; To find \\(P(Y = k)\\) we can use the function dpois(x, lambda) where: x is the quantile of interest, and lambda is the constant mean rate. # Find P(Y = 5) with Y~Pois(8) prob_pois &lt;- dpois(5, 8) paste0( &quot;P(Y = 5) with Y~Pois(5, 8) is &quot;, format(prob_pois, digits = 4), &quot;.&quot; ) ## [1] &quot;P(Y = 5) with Y~Pois(5, 8) is 0.0916.&quot; If we wanted to find \\(P(k_1 \\leq Y \\leq k_2)\\) we can also use the function dpois(x, lambda) with x entered as a vector and summing over the output. # Find P(4 &lt;= Y &lt;= 6) where Y~Pois(8) prob_pois &lt;- sum(dpois(4:6, 5)) paste0( &quot;P(4 &lt;= Y &lt;= 6) with Y~Pois(8) is &quot;, format(prob_pois, digits = 4), &quot;.&quot; ) ## [1] &quot;P(4 &lt;= Y &lt;= 6) with Y~Pois(8) is 0.4972.&quot; When we are interested in finding the yth percentile we can use the quantile function, qpois(p, lambda) where: p is the percentile of interest, and lambda is the constant mean rate. # Find the 99th percentile of Y~Poisson(8) percentile_99 &lt;- qpois(0.99, 8) paste0( &quot;The 99th percentile of Y~Pois(8) is &quot;, percentile_99, &quot;.&quot; ) ## [1] &quot;The 99th percentile of Y~Pois(8) is 15.&quot; Finally, we finish with a plot of Poisson distributions. We note that for large \\(\\lambda\\) the Poisson distribution is well approximated by the normal distribution with \\(\\mu = \\lambda\\) and \\(\\sigma^2 = \\lambda\\). We can approximate the Poisson distribution with this normal distribution after allowing for a continuity correction: \\[F_{Pois}(x; \\lambda) \\approx F_{normal}(x + 0.5; \\mu = \\lambda, \\sigma^2 = \\lambda)\\] Here we have replaced \\(P(X \\leq x)\\) with \\(P(X \\leq x + 0.5)\\) on account of the continuity correction. # Plot the distribution function of X, Y, Z with: # X,Y,Z~Pois(lambda) for various lambda library(ggplot2) set.seed(42) lambdas &lt;- c( &quot;lambda = 8&quot;, &quot;lambda = 16&quot;, &quot;lambda = 32&quot; ) df &lt;- data.frame( dist_types = rep(lambdas, each = 1000), dist_values = c( rpois(1000, 8), rpois(1000, 16), rpois(1000, 32) ) ) ggplot(df, aes(x = dist_values, fill = dist_types)) + geom_histogram(binwidth = 1, alpha = 0.75, position = &quot;identity&quot;) + xlab(&quot;Distribution values&quot;) + ylab(&quot;Count&quot;) + theme_minimal() + labs(fill = &quot;&quot;) + scale_fill_discrete(breaks = lambdas) + ggtitle(&quot;Histogram of Poisson distributions for various lambda&quot;) 2.7 Uniform distribution Our final discrete probability distribution concerns the uniform distribution (\\(Z\\)) defined over a finite set \\({a, b}\\). This is one discrete probability distribution which is not part of the source code for the stats package in base R. We can use rdunif(n, b, a) function within the purrr package7 to generate random deviates following the discrete uniform distribution. We have: \\[P(Z = k) = \\frac{1}{b - a + 1}\\] As an example we can simulate a die throw using \\(Z \\sim \\mathcal{U}(1, 6)\\) as follows: # Generate 5 random deviates on Z~Unif(1, 6) set.seed(42) purrr::rdunif(5, 6, 1) ## [1] 1 5 1 1 2 To find \\(P(Z \\leq k)\\) we can calculate this using the CDF: \\[P(Z \\leq k) = \\frac{\\lfloor k \\rfloor - a + 1}{b - a + 1}\\] # Find P(Z &lt;= 2) with Z~Unif(1, 6) prob_uniform &lt;- (2 - 1 + 1) / (6 - 1 + 1) paste0( &quot;P(Z &lt;= 2) with Z~Unif(1, 6) is &quot;, format(prob_uniform, digits = 4), &quot;.&quot; ) ## [1] &quot;P(Z &lt;= 2) with Z~Unif(1, 6) is 0.3333.&quot; To find \\(P(Z = k)\\) we can use the PMF, \\(P(Z = k) = \\frac{1}{b - a + 1}\\): # Find P(Z = 2) with Z~Unif(1, 6) prob_uniform &lt;- 1 / (6 - 1 + 1) paste0( &quot;P(Z = 2) with Z~Unif(1, 6) is &quot;, format(prob_uniform, digits = 4), &quot;.&quot; ) ## [1] &quot;P(Z = 2) with Z~Unif(1, 6) is 0.1667.&quot; To find \\(P(k_1 \\leq Z \\leq k_2)\\) we can sum over the PMFs: # Find P(3 &lt;= Z &lt;= 5) where Z~Unif(1, 6) # P(Z = k) is constant for all k: prob_uniform &lt;- 1 / (6 - 1 + 1) # We have k = 3, 4, 5: prob_uniform &lt;- prob_uniform * 3 paste0( &quot;P(3 &lt;= Z &lt;= 5) with Z~Unif(1, 6) is &quot;, format(prob_uniform, digits = 4), &quot;.&quot; ) ## [1] &quot;P(3 &lt;= Z &lt;= 5) with Z~Unif(1, 6) is 0.5.&quot; When we are interested in finding the zth percentile we can use the quantile function \\(F^{-1}_{Z}(p)\\): \\[F^{-1}(p) = \\lfloor (b - a + 1)p \\rfloor - 1\\] # Plot the distribution function of Z~Uniform(1, 6) percentile_99 &lt;- floor((6 - 1 + 1)*0.99) paste0( &quot;The 99th percentile of Z~Unif(1, 6) is &quot;, percentile_99, &quot;.&quot; ) ## [1] &quot;The 99th percentile of Z~Unif(1, 6) is 5.&quot; Finally, we finish with a plot of uniform distributions for various inputs. # Plot the distribution function of X, Y, Z with: # X,Y,Z~Unif(a, b) for various {a, b} library(ggplot2) set.seed(42) finite_sets &lt;- c( &quot;a = 1, b = 6&quot;, # 1 die &quot;a = 1, b = 12&quot;, # 2 dice &quot;a = 1, b = 20&quot; # d20 die from D&amp;D ) df &lt;- data.frame( dist_types = rep(finite_sets, each = 5000), dist_values = c( purrr::rdunif(5000, 1 , 6), purrr::rdunif(5000, 1, 12), purrr::rdunif(5000, 1, 20) ) ) ggplot(df, aes(x = dist_values, fill = dist_types)) + geom_histogram(binwidth = 1, alpha = 0.75, position = &quot;identity&quot;) + xlab(&quot;Distribution values&quot;) + ylab(&quot;Count&quot;) + theme_minimal() + labs(fill = &quot;&quot;) + scale_fill_discrete(breaks = finite_sets) + ggtitle(&quot;Histogram of uniform distributions for various inputs&quot;) R Practice We finish with a comprehensive example of an univariate discrete distribution question in R. For inspiration consider the BBC’s cookbook that highlights how to create their distinctive graphics predominately using ggplot2.↩︎ This probability was chosen based on Table 1 from Sex ratios at birth in the United Kingdom, 2014-18 from the Department of Health &amp; Social Care.↩︎ This turns out to be the number of live births in Guildford in 2019, as per Table 3 of the ONS’ Birth Summary Tables, England and Wales, 2019. Source: Office for National Statistics under the Open Government Licence.↩︎ purrr is a part of the tidyverse suite of packages↩︎ "],
["continous-probability-distributions.html", "Chapter 3 Continous Probability Distributions Learning Objectives Theory 3.1 In-built probability distributions Continous probability distributions covered 3.2 Normal distribution 3.3 Lognormal distribution 3.4 Exponential distribution 3.5 Gamma distribution 3.6 \\(\\chi^2\\) distribution 3.7 Student’s \\(t\\) distribution 3.8 \\(F\\) distribution 3.9 Beta distribution 3.10 Uniform distribution 3.11 Inverse transform method R Practice", " Chapter 3 Continous Probability Distributions Learning Objectives Define and explain the key characteristics of the continous distributions: normal, lognormal, exponential, gamma, chi-square, \\(t\\), \\(F\\), beta and uniform on an interval. Evaluate probabilities and quantiles associated with such distributions. Generate discrete and continous random variables using the inverse transform method. Theory A reminder: R was designed to be used for statistical computing - so it handles randomness well! Using R we can guarantee reproducibility (and enhance sharability) by using the function set.seed(seed) where seed is a single value integer. Using this approach we guarantee the generation of the same sequence of random numbers everytime we call this function. Use ?set.seed to learn more about this function. 3.1 In-built probability distributions A recap: R has in-built functions for probability distributions: d&lt;distribution-name&gt; \\(:=\\) density (“PDF”), i.e. \\(f_X(x)\\) p&lt;distribution-name&gt; \\(:=\\) probability distribution cumulative function (“CDF”), i.e. \\(F_X(x) =\\boldsymbol{P}(X \\leq x)\\) q&lt;distribution-name&gt; \\(:=\\) quantile function, i.e. return \\(x\\) such that \\(\\boldsymbol{P}(X \\leq x) = p\\) r&lt;distribution-name&gt; \\(:=\\) random deviates, i.e. (psuedo) random number generator for a given distribution Where &lt;distribution-name&gt; \\(=\\) Normal, uniform, lognormal, Student’s \\(t\\), Poisson, binormal, Weibull … see ?distributions() for more information To give some quick examples (we will further explore these in more detail later in this chapter): R Code Definition rnorm(1) Generates \\(x_1\\) where \\(X \\sim \\mathcal{N}(0,\\,1)\\) rnorm(y, mean=10, sd=2) Generates \\(\\{y_1,\\,y_2,\\,\\dots\\}\\) with \\(Y \\sim \\mathcal{N}(10,\\,2^2)\\) runif(3, min=5, max=10) Generates \\(\\{z_1,\\,z_2,\\,z_3\\}\\) where \\(Z \\sim \\mathcal{U}(5,\\,10)\\) dbinom(4, size=5, prob=0.5) Computes \\(\\boldsymbol{P}(X = 4)\\) where \\(X \\sim Bin(5,\\,0.5)\\) pgamma(0.2, shape=2, rate=2) Computes \\(F_Y(0.2)\\) where \\(Y \\sim \\mathcal{\\Gamma}(2,\\,2)\\), i.e. \\(\\boldsymbol{P}(Y\\leq 0.2)\\) qexp(0.5, rate = 2) Determines smallest value of \\(z\\) for \\(\\boldsymbol{P}(Z \\leq z) = 0.5\\) where \\(Z \\sim Exp(2)\\) Continous probability distributions covered We will consider how to interact with the following continous probability distributions in R: Normal Lognormal Exponential Gamma \\(\\chi^2\\) Student’s \\(t\\) \\(F\\) Beta Uniform (on an interval) For each distribution above we will determine how to calculate: A random deviate following the discrete distribution \\(X\\), The probability density function (“PDF”), \\(P(k_1 \\leq X \\leq k_2)\\) for distribution \\(X\\) over the range \\([k_1,\\,k_2]\\), The cumulative distribution function (“CDF”), \\(P(X \\leq k)\\), and The quantile function to find \\(k\\) representing the value such that \\(P(X \\leq k) = p\\), i.e. the pth percentile. We will finish off with a plot of the distribution. 3.2 Normal distribution We start with generating random deviates from the Normal distribution. If we are interested in the standard normal, R helpfully has default argument values for \\(\\mu = 0\\) and \\(\\sigma = 0\\) so the function call is very concise: # Generate random deviates set.seed(42) rnorm(5) ## [1] 1.3709584 -0.5646982 0.3631284 0.6328626 0.4042683 We can also specify our own values of \\(\\mu\\) and \\(\\sigma\\). With R we need to remember that the \\(\\sigma\\) argument corresponds to the standard deviation, not the variance. # Generate random deviates set.seed(42) rnorm(5, 10, 2) ## [1] 12.741917 8.870604 10.726257 11.265725 10.808537 We next look at the cumulative distribution function for a Normal distribution. In R we can calculate this using pnorm(q, mean, sd) where: q is the quantile of interest, mean is the mean, and sd is standard deviation. # Calculate P(X &lt;= 2) for X~N(0,1) Next we want to find the xth percentile of \\(X \\sim N(\\mu, \\sigma)\\). We use the quantile function, qnorm(mu, sigma). # Find the 99th percentile for X~N(10,2) percentile_99 &lt;- qnorm(0.99, 10, 2) paste0( &quot;The 99th percentile of X~N(10, 2) is &quot;, format(percentile_99, digits = 4), &quot;.&quot; ) ## [1] &quot;The 99th percentile of X~N(10, 2) is 14.65.&quot; As is customary we finish with a plot of the normal distribution. # Plot the distribution function of X~Normal(mu =, sigma = ) 3.3 Lognormal distribution # Plot the distribution function of Y~Lognormal() 3.4 Exponential distribution # Plot the distribution function of Z~Exp() 3.5 Gamma distribution # Plot the distribution of X~Gamma() 3.6 \\(\\chi^2\\) distribution # Plot the distribution of Y~Chi-Square() 3.7 Student’s \\(t\\) distribution # Plot the distribution of Z~t() 3.8 \\(F\\) distribution # Plot the distribution of X~F() 3.9 Beta distribution # Plot the distribution of Y~Beta() 3.10 Uniform distribution # Plot the distribution of Z~Uniform() 3.11 Inverse transform method The inverse transform method is a way to generate psuedo-random numbers from any probability distribution. One possible algorithm is as follows: Generate a random number \\(u\\) from $U (0, 1) Find the inverse of the desired cumulative distribution function, \\(F^{-1}_X(x)\\) Compute \\(X = F^{-1}_X(u)\\) Suppose we wanted to draw 10,000 random numbers from \\(X \\sim Exp(\\lambda = 2)\\). In order to use the inverse transform method we first need to find the inverse of the CDF. For any \\(X \\sim Exp(\\lambda)\\), the inverse of the CDF is \\(\\frac{-log(1-x)}{\\lambda}\\). We can thus use the inverse transform algorithm to generate random deviates following \\(X \\sim Exp(2)\\): # Step 0 - to guarantee reproducibility set.seed(42) # Step 1 - generate 10,000 random deviates from U[0,1] u &lt;- runif(10000) # Step 2 - find the inverse of the CDF: 1 - exp(-lambda.x) # Inverse of CDF = -log(1 - x) / lambda # Step 3 - compute X using the inverse of the CDF [from step 2] and the random deviates u [from step 1] x &lt;- -log(1 - u) / 2 # Plot the resulting x deviates library(ggplot2) df &lt;- data.frame(x = x) ggplot(df, aes(x=x)) + geom_histogram(binwidth = 0.5, colour=&quot;black&quot;, fill=&quot;white&quot;) R Practice We finish with a comprehensive example of an univariate continous distribution question in R. "],
["joint-and-conditional-distributions.html", "Chapter 4 Joint and Conditional Distributions Learning Objectives Theory R Practice", " Chapter 4 Joint and Conditional Distributions Learning Objectives Explain what is meant by jointly distributed random variables, marginal distributions and conditional distributions. Define the probability function/density function of a marginal distribution and of a conditional distribution. Specify the conditions under which random variables are independent. Define the expected value of a function of two jointly distributed random variables, the covariance and correlation coefficient between two variables, and calculate such quantities. Define the probability function/density function of the sum of two independent random variables as the convolution of two functions. Derive the mean and variance of linear combinations of random variables. Theory R Practice "],
["central-limit-theorem.html", "Chapter 5 Central Limit Theorem Learning Objectives Theory R Practice", " Chapter 5 Central Limit Theorem Learning Objectives State the Central Limit Theorem for a sequence of independent, identically distributed random variables. Generate simulated samples from a given distribution and compare the sampling distribution with the Normal. Theory R Practice "],
["data-analysis.html", "Chapter 6 Data Analysis Learning Objectives Theory R Practice", " Chapter 6 Data Analysis Learning Objectives Describe the possible aims of a data analysis (e.g. descriptive, inferential, and predictive). Describe the stages of conducting a data analysis to solve real-world problems in a scientific manner and describe tools suitable for each stage. Describe sources of data and explain the characteristics of different data sources, including extremely large data sets. Explain the meaning and value of reproducible research and describe the elements required to ensure a data analysis is reproducible. Theory R Practice "],
["exploratory-data-analysis.html", "Chapter 7 Exploratory Data Analysis Learning Objectives Theory R Practice", " Chapter 7 Exploratory Data Analysis Learning Objectives Describe the purpose of exploratory data analysis. Use appropriate tools to calculate suitable summary statistics and undertake exploratory data visualizations. Define and calculate Pearson’s, Spearman’s and Kendall’s measures of correlation for bivariate data, explain their interpretation and perform statistical inference as appropriate. Use Principal Components Analysis to reduce the dimensionality of a complex data set. Theory R Practice "],
["random-sampling.html", "Chapter 8 Random Sampling Learning Objectives Theory R Practice", " Chapter 8 Random Sampling Learning Objectives Explain what is meant by a sample, a population and statistical inference. Define a random sample from a distribution of a random variable. Explain what is meant by a statistic and its sampling distribution. Determine the mean and variance of a sample mean and the mean of a sample variance in terms of the population mean, variance and sample size. State and use the basic sampling distributions for the sample mean and the sample variance for random samples from a normal distribution. State and use the distribution of the \\(t\\)-statistic for random samples from a normal distribution. State and use the \\(F\\) distribution for the ratio of two sample variances from independent samples taken from normal distributions. Theory R Practice "],
["estimation-and-estimators.html", "Chapter 9 Estimation and estimators Learning Objectives Theory R Practice", " Chapter 9 Estimation and estimators Learning Objectives Describe and apply the method of moments for constructing estimators of population parameters. Describe and apply the method of maximum likelihood for constructing estimators of population parameters. Define the terms: efficiency, bias, consistency and mean squared error. Define and apply the property of unbiasedness of an estimator. Define the mean square error of an estimator, and use it to compare estimators. Describe and apply the asymptotic distribution of maximum likelihood estimators. Use the bootstrap method to estimate properties of an estimator. Theory R Practice "],
["confidence-intervals.html", "Chapter 10 Confidence Intervals Learning Objectives Theory R Practice", " Chapter 10 Confidence Intervals Learning Objectives Define in general terms a confidence interval for an unknown parameter of a distribution based on a random sample. Derive a confidence interval for an unknown parameter using a given sampling distribution. Calculate confidence intervals for the mean and the variance of a normal distribution. Calculate confidence intervals for a binomial probability and a Poisson mean, including the use of the normal approximation in both cases. Calculate confidence intervals for two-sample situations involving the normal distribution, and the binomial and Poisson distributions using the normal approximation. Calculate confidence intervals for a difference between two means from paired data. Use the bootstrap method to obtain confidence intervals. Theory R Practice "],
["hypothesis-testing.html", "Chapter 11 Hypothesis Testing Learning Objectives Theory R Practice", " Chapter 11 Hypothesis Testing Learning Objectives Explain what is meant by the terms null and alternative hypotheses, simple and composite hypotheses, type I and type II errors, test statistic, likelihood ratio, critical region, level of significance, probability-value and power of a test. Apply basic tests for the one-sample and two-sample situations involving the normal, binomial and Poisson distributions, and apply basic tests for paired data. Apply the permutation approach to non-parametric hypothesis tests. Theory R Practice "],
["goodness-of-fit.html", "Chapter 12 Goodness of Fit Learning Objectives Theory R Practice", " Chapter 12 Goodness of Fit Learning Objectives Use a chi-square test to test the hypothesis that a random sample is from a particular distribution, including cases where parameters are unknown. Explain what is meant by a contingency (or two-way) table, and use a chi-square test to test the independence of two classification criteria. Theory R Practice "],
["linear-regression.html", "Chapter 13 Linear Regression Learning Objectives Theory R Practice", " Chapter 13 Linear Regression Learning Objectives Explain what is meant by response and explanatory variables. State the simple regression model (with a single explanatory variable). Derive the least squares estimates of the slope and intercept parameters in a simple linear regression model. Use R to fit a simple linear regression model to a data set and interpret the output. Perform statistical inference on the slope parameter. Describe the use of measures of goodness of fit of a linear regression model. Use a fitted linear relationship to predict a mean response or an individual response with confidence limits. Use residuals to check the suitability and validity of a linear regression model. State the multiple linear regression model (with several explanatory variables). Use R to fit a multiple linear regression model to a data set and interpret the output. Use measures of model fit to select an appropriate set of explanatory variables. Theory R Practice "],
["generalised-linear-models.html", "Chapter 14 Generalised Linear Models Learning Objectives Theory R Practice", " Chapter 14 Generalised Linear Models Learning Objectives Define an exponential family of distributions. Show that the following distributions may be written in this form: binomial, Poisson, exponential, gamma, normal. State the mean and variance for an exponential family, and define the variance function and the scale parameter. Derive these quantities for the distributions above. Explain what is meant by the link function and the canonical link function, referring to the distributions above. Explain what is meant by a variable, a factor taking categorical values and an interaction term. Define the linear predictor, illustrating its form for simple models, including polynomial models and models involving factors. Define the deviance and scaled deviance and state how the parameters of a generalised linear model may be estimated. Describe how a suitable model may be chosen by using an analysis of deviance and by examining the significance of the parameters. Define the Pearson and deviance residuals and describe how they may be used. Apply statistical tests to determine the acceptability of a fitted model: Pearson’s chi-square test and the likelihood ratio test Fit a generalised linear model to a data set and interpret the output. Theory R Practice "],
["bayesian-statistics.html", "Chapter 15 Bayesian Statistics Learning Objectives Theory R Practice", " Chapter 15 Bayesian Statistics Learning Objectives Use Bayes’ theorem to calculate simple conditional probabilities. Explain what is meant by a prior distribution, a posterior distribution and a conjugate prior distribution. Derive the posterior distribution for a parameter in simple cases. Explain what is meant by a loss function. Use simple loss functions to derive Bayesian estimates of parameters. Explain what is meant by the credibility premium formula and describe the role played by the credibility factor. Explain the Bayesian approach to credibility theory and use it to derive credibility premiums in simple cases. Explain the empirical Bayes approach to credibility theory and use it to derive credibility premiums in simple cases. Explain the differences between the two approaches and state the assumptions underlying each of them. Theory R Practice "]
]
